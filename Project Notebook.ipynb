{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4d3a3-aba4-4666-a923-c8dd2aa60731",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Checking Folder Existence\n",
    "This code snippet checks whether a specified folder exists on the system. The path to the folder is defined, and the `os.path.exists()` function is used to verify its existence. If the folder is found, it prints \"Folder found!\" Otherwise, it prints \"Folder not found. Please check the path.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea559ae8-b08a-4e48-a898-8af8e5188572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to the new folder location\n",
    "folder_path = r\"OneDrive/Desktop/WVU/CYBR_520/hard_ham\"\n",
    "\n",
    "# Check if the path exists\n",
    "if os.path.exists(folder_path):\n",
    "    print(\"Folder found!\")\n",
    "else:\n",
    "    print(\"Folder not found. Please check the path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873669a9-8731-4c59-96b6-a36d432ac40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process Email Files in a Folder and Save as a CSV\n",
    "This code performs the following steps:\n",
    "1. **Folder Path Setup**:\n",
    "   - Sets the path to a folder containing individual email files.\n",
    "\n",
    "2. **Reading Email Content**:\n",
    "   - Iterates through all files in the specified folder using `os.listdir()`.\n",
    "   - Reads the content of each file and appends it to a list named `emails`.\n",
    "\n",
    "3. **Labeling Emails**:\n",
    "   - Assigns the label \"ham\" to all emails (assuming the folder contains only ham emails).\n",
    "\n",
    "4. **Creating a DataFrame**:\n",
    "   - Combines the email content and labels into a pandas DataFrame with columns `text` (email content) and `label` (email category).\n",
    "\n",
    "5. **Saving as CSV**:\n",
    "   - Saves the DataFrame to a CSV file named `hard_ham_emails.csv`.\n",
    "   - Prints a confirmation message with the total number of emails processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7731228-7a35-47ae-805e-2187df8469c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the correct path to the folder containing the email files\n",
    "folder_path = r\"OneDrive/Desktop/WVU/CYBR_520/hard_ham\"\n",
    "\n",
    "emails = []\n",
    "labels = []  # Assuming these are \"ham\" emails\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Open each file and read its content\n",
    "    with open(file_path, 'r', encoding='latin1') as file:\n",
    "        content = file.read()\n",
    "        emails.append(content)\n",
    "        labels.append(\"ham\")  # Label all these emails as \"ham\"\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df = pd.DataFrame({\n",
    "    'text': emails,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv = 'hard_ham_emails.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"CSV file '{output_csv}' created successfully with {len(df)} emails.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea9d04-36bf-4bfe-84fe-d909c8833cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get Current Working Directory\n",
    "This code snippet:\n",
    "1. **Imports the `os` module**: A built-in Python module for interacting with the operating system.\n",
    "2. **Prints the Current Working Directory (CWD)**:\n",
    "   - Uses the `os.getcwd()` function to retrieve the path of the directory where the Python script or Jupyter Notebook is currently running.\n",
    "   - This is useful for confirming the current directory before performing file operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77d899-0675-4831-bb65-41dd792ea106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40562146-4c5e-4ef1-9116-9710220881a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save DataFrame to a CSV File\n",
    "This code saves the processed DataFrame to a specified file path as a CSV. \n",
    "\n",
    "1. **Specify the Output Path**:\n",
    "   - The variable `output_csv` stores the file path where the CSV will be saved.\n",
    "   - Paths can be either absolute (full path from the root directory) or relative (path relative to the current working directory).\n",
    "\n",
    "2. **Save the DataFrame**:\n",
    "   - The `df.to_csv(output_csv, index=False)` function writes the DataFrame to the CSV file.\n",
    "   - The `index=False` parameter ensures that the DataFrame index is not included in the CSV file.\n",
    "\n",
    "### Notes:\n",
    "- The first code snippet uses an **absolute path** to save the file.\n",
    "- The second code snippet uses a **relative path** to save the file.\n",
    "- Ensure the path specified exists or is writable to avoid errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0194bbb6-20b3-4099-a62a-f168460c877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = r\"C:\\Users\\cjvan\\OneDrive\\Desktop\\WVU\\CYBR_520\\hard_ham_emails.csv\"\n",
    "df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca8691-ffe9-4a26-a137-97cef5b94aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = r\"OneDrive/Desktop/WVU/CYBR_520/hard_ham_emails.csv\"\n",
    "df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea2a7df-df71-435e-ab46-c2f1885604c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Verify Folder Path Existence\n",
    "This code verifies whether a specific folder exists in the filesystem. \n",
    "\n",
    "1. **Folder Path Setup**:\n",
    "   - The `folder_path` variable stores the path to the folder being checked.\n",
    "   - In this example, the path points to a folder named `spam_2` in the `CYBR_520` directory.\n",
    "\n",
    "2. **Check Folder Existence**:\n",
    "   - The `os.path.exists()` function checks if the specified folder path exists.\n",
    "   - If the folder exists, it prints \"Folder found!\".\n",
    "   - If the folder does not exist, it prints \"Folder not found. Please check the path.\"\n",
    "\n",
    "### Usage:\n",
    "- Use this snippet to ensure the folder path is correct before performing operations such as reading files or saving data.\n",
    "- Adjust the `folder_path` variable to match the location of your desired folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59501879-f3ac-4ed1-9366-e8ec943ec083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set the path to the new folder location\n",
    "folder_path = r\"OneDrive/Desktop/WVU/CYBR_520/spam_2\"\n",
    "\n",
    "# Check if the path exists\n",
    "if os.path.exists(folder_path):\n",
    "    print(\"Folder found!\")\n",
    "else:\n",
    "    print(\"Folder not found. Please check the path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7aceb09-c9f1-467c-b981-7c9b2bd0db41",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process and Save Spam Emails to a CSV File\n",
    "This code processes all email files in the `spam_2` folder and saves them into a labeled CSV file.\n",
    "\n",
    "1. **Folder Path Setup**:\n",
    "   - The `folder_path` variable specifies the directory containing the spam email files.\n",
    "\n",
    "2. **Read Email Files**:\n",
    "   - Iterates through each file in the folder using `os.listdir(folder_path)`.\n",
    "   - Opens each file with `open()` and reads its content into a list called `emails`.\n",
    "   - Each email is labeled as \"spam\" and appended to the `labels` list.\n",
    "\n",
    "3. **Create a DataFrame**:\n",
    "   - Combines the email content (`emails`) and labels (`labels`) into a pandas DataFrame with columns:\n",
    "     - `text`: Contains the email content.\n",
    "     - `label`: Contains the corresponding label (\"spam\").\n",
    "\n",
    "4. **Save to CSV**:\n",
    "   - Writes the DataFrame to a CSV file named `spam_2_emails.csv` using `df.to_csv()`.\n",
    "   - Prints a confirmation message indicating the number of processed emails and the file's location.\n",
    "   \n",
    "### Notes:\n",
    "- The `encoding='latin1'` parameter handles possible encoding issues in email files.\n",
    "- The output CSV can be used for further analysis or as input to machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716d74a9-2878-4780-9f2b-ebbce157cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Set the correct path to the folder containing the email files\n",
    "folder_path = r\"OneDrive/Desktop/WVU/CYBR_520/spam_2\"\n",
    "\n",
    "emails = []\n",
    "labels = []  # Assuming these are \"ham\" emails\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    # Open each file and read its content\n",
    "    with open(file_path, 'r', encoding='latin1') as file:\n",
    "        content = file.read()\n",
    "        emails.append(content)\n",
    "        labels.append(\"spam\")  # Label all these emails as \"spam\"\n",
    "\n",
    "# Create a DataFrame from the lists\n",
    "df = pd.DataFrame({\n",
    "    'text': emails,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv = 'spam_2_emails.csv'\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"CSV file '{output_csv}' created successfully with {len(df)} emails.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1512054b-26e7-4f5e-8915-1fdc8c78b163",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = r\"C:\\Users\\cjvan\\OneDrive\\Desktop\\WVU\\CYBR_520\\spam_2_emails.csv\"\n",
    "df.to_csv(output_csv, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675368fb-2530-426f-8090-07a25dfd272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Combine Ham and Spam Email Datasets into a Single CSV File\n",
    "This code combines two separate email datasets (`ham` and `spam`) into a unified dataset and saves it as a new CSV file.\n",
    "\n",
    "1. **Load Datasets**:\n",
    "   - `hard_ham_emails.csv`: Contains emails labeled as \"ham.\"\n",
    "   - `spam_2_emails.csv`: Contains emails labeled as \"spam.\"\n",
    "   - Both files are loaded into pandas DataFrames using `pd.read_csv()`.\n",
    "\n",
    "2. **Combine Datasets**:\n",
    "   - Uses `pd.concat()` to merge the two DataFrames (`ham_df` and `spam_df`) into a single DataFrame, `combined_df`.\n",
    "   - The `ignore_index=True` parameter resets the index in the combined dataset for consistency.\n",
    "\n",
    "3. **Save Combined Dataset**:\n",
    "   - Saves the merged DataFrame to a new CSV file named `BERT_Emails.csv` using `to_csv()`.\n",
    "   - Prints confirmation messages indicating the total number of emails in the combined dataset and the name of the saved file.\n",
    "\n",
    "### Notes:\n",
    "- The combined dataset can be used as input for text classification models, such as a BERT-based model.\n",
    "- Ensure the file paths for the input datasets are correct before running the code.\n",
    "- This code is part of a preprocessing pipeline to prepare data for further machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdce734-3935-4f41-81df-d6e0c92966b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the ham and spam datasets\n",
    "ham_df = pd.read_csv(r\"hard_ham_emails.csv\")\n",
    "spam_df = pd.read_csv(r\"spam_2_emails.csv\")\n",
    "\n",
    "# Combine the datasets\n",
    "combined_df = pd.concat([ham_df, spam_df], ignore_index=True)\n",
    "print(\"Dataset combined successfully with\", len(combined_df), \"total emails.\")\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_csv_path = r\"BERT_Emails.csv\"\n",
    "combined_df = pd.concat([ham_df, spam_df], ignore_index=True)\n",
    "print(f\"Combined CSV file saved as '{combined_csv_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db81092-9905-49f7-9f0c-242129d98ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load and Inspect the Combined Dataset\n",
    "This code loads the combined dataset of ham and spam emails (`BERT_Emails.csv`) and inspects its structure.\n",
    "\n",
    "1. **Load the Dataset**:\n",
    "   - The file `BERT_Emails.csv` is read into a pandas DataFrame (`df`) using `pd.read_csv()`.\n",
    "\n",
    "2. **Inspect the Dataset**:\n",
    "   - `df.head()`: Displays the first five rows of the DataFrame to provide a preview of the data.\n",
    "   - `df.columns`: Prints the column names of the DataFrame to confirm its structure.\n",
    "\n",
    "### Notes:\n",
    "- The dataset includes two columns:\n",
    "  - `text`: Contains the email content.\n",
    "  - `label`: Indicates the classification (`ham` or `spam`).\n",
    "- This step ensures that the dataset is loaded correctly before performing further operations, such as preprocessing or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f584f1a-7ea4-4807-97f6-07b689b30055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text label\n",
      "0  Return-Path: Fool@motleyfool.com\\nDelivery-Dat...   ham\n",
      "1  Return-Path: <malcolm-sweeps@mrichi.com>\\nDeli...   ham\n",
      "2  From nic@starflung.com  Mon Jun 24 17:06:54 20...   ham\n",
      "3  Received: from bran.mc.mpls.visi.com (bran.mc....   ham\n",
      "4  Return-Path: <iso17799@securityrisk.co.uk>\\nRe...   ham\n",
      "Index(['text', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"BERT_emails.csv\")\n",
    "\n",
    "# Inspect the dataset\n",
    "print(df.head())\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7f6af6-d008-4159-8e38-eb15a49de8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Context: Running BERT and DNN Models on Email Dataset\n",
    "Previously, we processed the `BERT_Emails.csv` dataset and successfully ran it through a BERT model to extract embeddings and a Deep Neural Network (DNN) for classification. However, the Jupyter Notebook crashed and did not save progress, requiring us to restart the process.\n",
    "\n",
    "### Current Challenges:\n",
    "1. **Resource Constraints**:\n",
    "   - Running the full BERT model on the dataset consistently crashes Jupyter Lab due to insufficient memory or compute resources.\n",
    "   - Attempts to use lighter BERT models, such as `google/electra-small-discriminator`, have also resulted in crashes.\n",
    "\n",
    "2. **Current Objective**:\n",
    "   - Reload the dataset.\n",
    "   - Split the data into training and testing sets.\n",
    "   - Attempt to reinitialize and run a lightweight transformer-based tokenizer and model for embedding generation.\n",
    "\n",
    "### Actions Taken:\n",
    "- Verified available system memory using `psutil`.\n",
    "- Tried using a smaller BERT model (`google/electra-small-discriminator`) to mitigate resource issues, but it failed to initialize correctly in Jupyter Lab.\n",
    "\n",
    "### Next Steps:\n",
    "- Explore even lighter transformer-based models (e.g., DistilBERT).\n",
    "- Consider using cloud-based resources or more powerful hardware for model execution.\n",
    "- Optimize preprocessing steps to reduce memory load before embedding generation.\n",
    "\n",
    "### Key Notes:\n",
    "- The code below attempts to re-run the process with a smaller model while ensuring the dataset is correctly split into training and testing sets for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13c594fb-0fef-4686-8bfe-e83ba93fecf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"BERT_emails.csv\")  # Replace with your actual file path\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04484267-5a80-4a93-b470-0929139ea1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Memory: 2829.10 MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "print(f\"Available Memory: {psutil.virtual_memory().available / (1024 * 1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb20295e-cb5e-4ff1-8b47-c4ba1d032b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc73b066-2c0e-4847-928b-a2d5255deead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffb9853bc9a492b8d19b3c441dce9df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579bea5500bb44df875b308de7584409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfb69ce1371443e2a5ddec079222f633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f625fb94f03b44ba99c5283b0c1fe765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57e83bc-10df-447f-9c14-40c600ed7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('google/electra-small-discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e9cf1-f092-4de5-9874-4c689c3275d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cf6ac5-1b85-4a1a-8763-cc5958a792af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reducing Email Datasets for Smaller Sample Sizes\n",
    "To address resource constraints and improve the likelihood of successfully running BERT on the email dataset, we decided to reduce the datasets to smaller sample sizes.\n",
    "\n",
    "1. **Ham Dataset**:\n",
    "   - Loaded `hard_ham_emails.csv` containing ham emails.\n",
    "   - Randomly sampled 35 emails from the dataset using `pandas.DataFrame.sample()`.\n",
    "   - Saved the reduced dataset to a new CSV file named `Reduced_Ham.csv`.\n",
    "\n",
    "2. **Spam Dataset**:\n",
    "   - Loaded `spam_2_emails.csv` containing spam emails.\n",
    "   - Randomly sampled 45 emails from the dataset.\n",
    "   - Saved the reduced dataset to a new CSV file named `Reduced_Spam.csv`.\n",
    "\n",
    "3. **Reason for Reduction**:\n",
    "   - BERT models require significant computational resources to process large datasets.\n",
    "   - Reducing the dataset size allows us to test the pipeline with smaller, manageable samples.\n",
    "\n",
    "### Next Steps:\n",
    "- Combine `Reduced_Ham.csv` and `Reduced_Spam.csv` into a single dataset for classification.\n",
    "- Re-run the BERT model and DNN classifier with these smaller datasets to verify functionality.\n",
    "\n",
    "### Notes:\n",
    "- The `random_state=42` parameter ensures reproducibility of the sampling process.\n",
    "- This approach provides a baseline for testing while avoiding crashes caused by insufficient memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "614b0950-e9d9-4927-b35e-906ddbeadcc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of 35 emails saved to Reduced_Ham.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'hard_ham_emails.csv'  # Update this with the correct file path\n",
    "emails_df = pd.read_csv(file_path)\n",
    "\n",
    "# Randomly sample 35 emails\n",
    "subset_emails = emails_df.sample(n=35, random_state=42)  # random_state ensures reproducibility\n",
    "\n",
    "# Save the subset to a new CSV file named \"Reduced_Ham.csv\"\n",
    "subset_path = 'Reduced_Ham.csv'\n",
    "subset_emails.to_csv(subset_path, index=False)\n",
    "\n",
    "print(f\"Subset of 35 emails saved to {subset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2deb74a8-d516-43c5-9775-ec12d39025a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset of 35 emails saved to Reduced_Spam.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'spam_2_emails.csv'  # Update this with the correct file path\n",
    "emails_df = pd.read_csv(file_path)\n",
    "\n",
    "# Randomly sample 35 emails\n",
    "subset_emails = emails_df.sample(n=45, random_state=42)  # random_state ensures reproducibility\n",
    "\n",
    "# Save the subset to a new CSV file named \"Reduced_Ham.csv\"\n",
    "subset_path = 'Reduced_Spam.csv'\n",
    "subset_emails.to_csv(subset_path, index=False)\n",
    "\n",
    "print(f\"Subset of 35 emails saved to {subset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd973d6a-5ff4-4cc2-a0f3-102c9d7c1b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset saved to Reduced_BERT.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Reduced_Ham and Reduced_Spam datasets\n",
    "ham_path = 'Reduced_Ham.csv'  # Update with actual file path if needed\n",
    "spam_path = 'Reduced_Spam.csv'  # Update with actual file path if needed\n",
    "\n",
    "reduced_ham = pd.read_csv(ham_path)\n",
    "reduced_spam = pd.read_csv(spam_path)\n",
    "\n",
    "# Combine the two datasets\n",
    "reduced_bert = pd.concat([reduced_ham, reduced_spam], ignore_index=True)\n",
    "\n",
    "# Save the combined dataset as Reduced_BERT.csv\n",
    "output_path = 'Reduced_BERT.csv'\n",
    "reduced_bert.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined dataset saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d35b8-bc89-4013-9022-f12db79e1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "The following commands use the !pip syntax to install the necessary Python libraries for this project. The !pip command allows you to run shell commands directly within a Jupyter Notebook environment. These libraries include:\n",
    "\n",
    "transformers: Provides tools and models for natural language processing (NLP) tasks, such as BERT.\n",
    "torch: PyTorch, a deep learning library for building and training neural networks.\n",
    "scikit-learn: A machine learning library for model evaluation and feature engineering.\n",
    "pandas: Used for data manipulation and analysis.\n",
    "numpy: Provides support for numerical computations.\n",
    "matplotlib: A library for creating data visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2a7b34f-38b5-40ff-bcdc-8e9e541ca422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/cjvandy21/.local/lib/python3.11/site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in /home/cjvandy21/.local/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/cjvandy21/.local/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/cjvandy21/.local/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cjvandy21/.local/lib/python3.11/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/cjvandy21/.local/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/cjvandy21/.local/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/cjvandy21/.local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/cjvandy21/.local/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/cjvandy21/.local/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/cjvandy21/.local/lib/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/cjvandy21/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/cjvandy21/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/cjvandy21/.local/lib/python3.11/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/cjvandy21/.local/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/cjvandy21/.local/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/cjvandy21/.local/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/cjvandy21/.local/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/lib/python3/dist-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/cjvandy21/.local/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/lib/python3/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/cjvandy21/.local/lib/python3.11/site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/cjvandy21/.local/lib/python3.11/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/cjvandy21/.local/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/cjvandy21/.local/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/cjvandy21/.local/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/cjvandy21/.local/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /home/cjvandy21/.local/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/cjvandy21/.local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/cjvandy21/.local/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/cjvandy21/.local/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/cjvandy21/.local/lib/python3.11/site-packages (1.26.4)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/cjvandy21/.local/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/cjvandy21/.local/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/cjvandy21/.local/lib/python3.11/site-packages (from matplotlib) (4.55.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/cjvandy21/.local/lib/python3.11/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/cjvandy21/.local/lib/python3.11/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/cjvandy21/.local/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/cjvandy21/.local/lib/python3.11/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/cjvandy21/.local/lib/python3.11/site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/cjvandy21/.local/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da0c5a5-4052-49d4-a5cc-7cbbadaaea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "***BERT Model Step 1*** \n",
    "### We are not having any issues to run this code, it is processing in Jupyter lab correctly\n",
    "    \n",
    "### Preprocessing Data for BERT Classification\n",
    "1. To ensure the BERT model processes the input text correctly, the dataset is preprocessed and tokenized. Below are the steps and their purposes:\n",
    "- Import Required Libraries:\n",
    "- Imported pandas for dataset handling.\n",
    "- Used train_test_split from sklearn for dividing the dataset into training and testing sets.\n",
    "- Loaded BertTokenizer from transformers to tokenize text data into BERT-compatible input.\n",
    "\n",
    "2. Loaded the Reduced_BERT.csv file containing email texts and their labels.\n",
    "- The dataset includes:\n",
    "- text: Input email content.\n",
    "- label: Classification labels (ham or spam).\n",
    "\n",
    "3. Map Labels to Integers:\n",
    "- Mapped ham to 0 and spam to 1 for binary classification\n",
    "\n",
    "4. Split Data into Training and Testing Sets:\n",
    "-Split the dataset into 80% training and 20% testing subsets using train_test_split\n",
    "\n",
    "5. Initialize BERT Tokenizer:\n",
    "- Loaded a pre-trained BERT tokenizer (bert-base-uncased) for converting text into tokens.\n",
    "\n",
    "6. Tokenize Data:\n",
    "- Tokenized both training and testing data using the tokenizer\n",
    "    -- truncation=True: Ensures text exceeding the maximum length is truncated.\n",
    "    -- padding=True: Pads sequences to uniform length.\n",
    "    -- max_length=512: Maximum token length allowed. We have also reduced this to 32, to see if it would prevent the kernal from crashing\n",
    "    -- return_tensors=\"pt\": Converts output into PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfbf21c8-95ad-4787-8bf4-52de02231208",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the Reduced_BERT.csv file\n",
    "data_path = 'Reduced_BERT.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Extract text and labels\n",
    "texts = data['text'].tolist()\n",
    "labels = data['label'].map({'ham': 0, 'spam': 1}).tolist()  # Map labels to 0 and 1\n",
    "\n",
    "# Split into train and test sets\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the data\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=32, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=32, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3cb401-5be2-47f4-924f-d3231ae24397",
   "metadata": {},
   "outputs": [],
   "source": [
    "*** BERT Model Step 2 ***\n",
    "\n",
    "###This step is causing the kernel to crash when running, but it did successfully execute once.\n",
    "\n",
    "1. Generating Embeddings Using Pre-Trained BERT\n",
    "- This step uses the pre-trained BERT model to generate embeddings (vector representations) for the training and test datasets. Below are the steps and their purposes:\n",
    "- Import Required Libraries:\n",
    "- Imported torch for handling tensor operations.\n",
    "- Loaded BertModel from transformers to use the pre-trained BERT model.\n",
    "- Load Pre-Trained BERT Model\n",
    "\n",
    "2. Loaded the bert-base-uncased model, a pre-trained BERT model provided by Hugging Face.\n",
    "- This model generates contextualized embeddings for input text.\n",
    "- Generate Embeddings for Training and Test Sets:\n",
    "\n",
    "###The embeddings for both the training and testing datasets are generated by passing the tokenized data through the pre-trained BERT model:\n",
    "\n",
    "3. Explanation of the Code:\n",
    "- torch.no_grad(): Disables gradient computation to save memory during inference.\n",
    "- last_hidden_state: Contains the embeddings for all tokens in the input text.\n",
    "- .mean(dim=1): Performs mean pooling to aggregate token embeddings into a single vector for each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3185338-3fcf-4be3-915b-bc1327fab668",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Generate embeddings for the training and test sets\n",
    "with torch.no_grad():\n",
    "    train_embeddings = bert_model(**train_encodings).last_hidden_state.mean(dim=1)  # Pooling\n",
    "    test_embeddings = bert_model(**test_encodings).last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d8064-4390-4fd8-939b-fdd34d7b6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the DNN model\n",
    "class DNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(DNNClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Model parameters\n",
    "input_dim = train_embeddings.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Binary classification: ham/spam\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "model = DNNClassifier(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert embeddings and labels to PyTorch tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Train the DNN\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(train_embeddings)\n",
    "    loss = criterion(outputs, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11746f0e-a37b-4b14-9eca-ef5fd96df577",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_embeddings).argmax(dim=1)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(test_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc0d84a-7084-4e85-838a-877c452526f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae699328-88ba-4780-844c-ce01e397c17a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
